{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of code is the reinforcement learning brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "\n",
    "Policy Gradient, Reinforcement Learning.\n",
    "\n",
    "Using:\n",
    "Tensorflow: 1.4\n",
    "gym: 0.8.0\n",
    "\n",
    "REINFORCE Monte Carlo Policy Gradients\n",
    "https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self,\n",
    "                 n_actions,\n",
    "                 n_features,\n",
    "                 learning_rate = 0.01,\n",
    "                 reward_decay = 0.9,\n",
    "                 output_graph = False):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs,self.ep_as,self.ep_rs = [],[],[]#used to record observation value, action and reward for the episode now.\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\",self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32,[None,self.n_features],name='observation')\n",
    "            self.tf_acts = tf.placeholder(tf.int32,[None,],name='actions_num')\n",
    "            self.tf_vt = tf.placeholder(tf.float32,[None,],name='actions_value')\n",
    "\n",
    "        layer = tf.layers.dense(\n",
    "            inputs = self.tf_obs,\n",
    "            units = 10,\n",
    "            activation= tf.nn.tanh,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "\n",
    "        all_act = tf.layers.dense(\n",
    "            inputs = layer,\n",
    "            units = self.n_actions,\n",
    "            activation = None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.3),\n",
    "            bias_initializer = tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        self.all_act_prob = tf.nn.softmax(all_act,name='act_prob')\n",
    "\n",
    "        with tf.name_scope('loss'):#loss function is cross entropy\n",
    "            #neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.all_act_prob,labels =self.tf_acts)\n",
    "\n",
    "            neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob) * tf.one_hot(indices=self.tf_acts,depth=self.n_actions),axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)\n",
    "\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        prob_weights = self.sess.run(self.all_act_prob,feed_dict={self.tf_obs:observation[np.newaxis,:]})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]),p=prob_weights.ravel())\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,s,a,r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        self.sess.run(self.train_op,feed_dict={\n",
    "            self.tf_obs:np.vstack(self.ep_obs),\n",
    "            self.tf_acts:np.array(self.ep_as),\n",
    "            self.tf_vt:discounted_ep_rs_norm,\n",
    "        })\n",
    "\n",
    "        self.ep_obs,self.ep_as,self.ep_rs = [],[],[]\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        # reserved 返回的是列表的反序，这样就得到了贴现求和值。\n",
    "        for t in reversed(range(0,len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
