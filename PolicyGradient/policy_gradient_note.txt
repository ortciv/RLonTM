Policy Gradient and why it is not performing properly:
1. Loss function: We always need a loss function which we can learn and thus find a good policy by the gradient of this function.
Starting with a straightforward though: if an action gets a better reward, we increase its possibility. Hence, we use function: loss= -log(prob)*vt where vt is the reward and prob is the current possibility of the action. The key here is vt is not the instant reward, instead vt should include future reward as well.

However, now the possibility returned by this model is somehow nan for some cases. This probably results from the loss function.

2. Input of the model includes: obsercation, action and rewards, which is the same for our model. Each of them is put into the system after an episode.


# Notes of Changes April 7:

Simple model replaced with model developed from supervised learning, however not sure how to use weights since the original model is only good for 10 bins

NaN issue fixed.  Issue was in the _discount_and_norm_rewards
	discounted_ep_rs /= np.std(discounted_ep_rs) + 1e-50 #1e-50 to avoid divide by 0 which is causing NaNs
