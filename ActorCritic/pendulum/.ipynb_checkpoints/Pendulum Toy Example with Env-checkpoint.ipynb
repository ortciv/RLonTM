{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"st..., outputs=Tensor(\"ou...)`\n",
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:102: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ou...)`\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69\n",
    "\n",
    "\"\"\"\n",
    "solving pendulum using actor-critic model\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "from keras.models import load_model\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "class ActorCritic:\n",
    "\tdef __init__(self, env, sess):\n",
    "\t\tself.env  = env\n",
    "\t\tself.sess = sess\n",
    "\n",
    "\t\tself.learning_rate = 0.001\n",
    "\t\tself.epsilon = 1.0\n",
    "\t\tself.epsilon_decay = .995\n",
    "\t\tself.gamma = .95\n",
    "\t\tself.tau   = .125\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                               Actor Model                             #\n",
    "\t\t# Chain rule: find the gradient of chaging the actor network params in  #\n",
    "\t\t# getting closest to the final value network predictions, i.e. de/dA    #\n",
    "\t\t# Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "\t\t# ===================================================================== #\n",
    "\n",
    "\t\tself.memory = deque(maxlen=2000)\n",
    "\t\tself.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "\t\t_, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "\t\tself.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "\t\t\t[None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "\t\t\n",
    "\t\tactor_model_weights = self.actor_model.trainable_weights\n",
    "\t\tself.actor_grads = tf.gradients(self.actor_model.output, \n",
    "\t\t\tactor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "\t\tgrads = zip(self.actor_grads, actor_model_weights)\n",
    "\t\tself.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                              Critic Model                             #\n",
    "\t\t# ===================================================================== #\t\t\n",
    "\n",
    "\t\tself.critic_state_input, self.critic_action_input, \\\n",
    "\t\t\tself.critic_model = self.create_critic_model()\n",
    "\t\t_, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "\t\tself.critic_grads = tf.gradients(self.critic_model.output, \n",
    "\t\t\tself.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "\t\t\n",
    "\t\t# Initialize for later gradient calculations\n",
    "\t\tself.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Definitions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef create_actor_model(self):\n",
    "\t\tstate_input = Input(shape=self.env.observation_space.shape, name='state_input')\n",
    "\t\th1 = Dense(24, activation='relu', name='h1')(state_input)\n",
    "\t\th2 = Dense(48, activation='relu', name='h2')(h1)\n",
    "\t\th3 = Dense(24, activation='relu', name='h3')(h2)\n",
    "\t\toutput = Dense(self.env.action_space.shape[0], activation='relu', name='output')(h3)\n",
    "\t\t\n",
    "\t\tmodel = Model(input=state_input, output=output)\n",
    "\t\tadam  = Adam(lr=0.001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, model\n",
    "\n",
    "\tdef create_critic_model(self):\n",
    "\t\tstate_input = Input(shape=self.env.observation_space.shape, name='state_input')\n",
    "\t\tstate_h1 = Dense(24, activation='relu', name='state_h1')(state_input)\n",
    "\t\tstate_h2 = Dense(48, name='state_h2')(state_h1)\n",
    "\t\t\n",
    "\t\taction_input = Input(shape=self.env.action_space.shape, name='action_input')\n",
    "\t\taction_h1    = Dense(48, name='action_h1')(action_input)\n",
    "\t\t\n",
    "\t\tmerged    = Add(name='merged')([state_h2, action_h1])\n",
    "\t\tmerged_h1 = Dense(24, activation='relu', name='merged_h1')(merged)\n",
    "\t\toutput = Dense(1, activation='relu', name='output')(merged_h1)\n",
    "\t\tmodel  = Model(input=[state_input,action_input], output=output)\n",
    "\t\t\n",
    "\t\tadam  = Adam(lr=0.001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, action_input, model\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                               Model Training                              #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef remember(self, cur_state, action, reward, new_state, done):\n",
    "\t\tself.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "\tdef _train_actor(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, _ = sample\n",
    "\t\t\tpredicted_action = self.actor_model.predict(cur_state)\n",
    "\t\t\tgrads = self.sess.run(self.critic_grads, feed_dict={\n",
    "\t\t\t\tself.critic_state_input:  cur_state,\n",
    "\t\t\t\tself.critic_action_input: predicted_action\n",
    "\t\t\t})[0]\n",
    "\n",
    "\t\t\tself.sess.run(self.optimize, feed_dict={\n",
    "\t\t\t\tself.actor_state_input: cur_state,\n",
    "\t\t\t\tself.actor_critic_grad: grads\n",
    "\t\t\t})\n",
    "            \n",
    "\tdef _train_critic(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, done = sample\n",
    "\t\t\tif not done:\n",
    "\t\t\t\ttarget_action = self.target_actor_model.predict(new_state)\n",
    "\t\t\t\tfuture_reward = self.target_critic_model.predict(\n",
    "\t\t\t\t\t[new_state, target_action])[0][0]\n",
    "\t\t\t\treward += self.gamma * future_reward\n",
    "\t\t\tself.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "\t\t\n",
    "\tdef train(self):\n",
    "\t\tbatch_size = 32\n",
    "\t\tif len(self.memory) < batch_size:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\trewards = []\n",
    "\t\tsamples = random.sample(self.memory, batch_size)\n",
    "\t\tself._train_critic(samples)\n",
    "\t\tself._train_actor(samples)\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                         Target Model Updating                             #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef _update_actor_target(self):\n",
    "\t\tactor_model_weights  = self.actor_model.get_weights()\n",
    "\t\tactor_target_weights = self.target_critic_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(actor_target_weights)):\n",
    "\t\t\tactor_target_weights[i] = actor_model_weights[i]\n",
    "\t\tself.target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "\tdef _update_critic_target(self):\n",
    "\t\tcritic_model_weights  = self.critic_model.get_weights()\n",
    "\t\tcritic_target_weights = self.critic_target_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(critic_target_weights)):\n",
    "\t\t\tcritic_target_weights[i] = critic_model_weights[i]\n",
    "\t\tself.critic_target_model.set_weights(critic_target_weights)\t\t\n",
    "\n",
    "\tdef update_target(self):\n",
    "\t\tself._update_actor_target()\n",
    "\t\tself._update_critic_target()\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Predictions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef act(self, cur_state):\n",
    "\t\tself.epsilon *= self.epsilon_decay\n",
    "\t\tif np.random.random() < self.epsilon:\n",
    "\t\t\treturn self.env.action_space.sample()\n",
    "\t\treturn self.actor_model.predict(cur_state)\n",
    "\n",
    "def main():\n",
    "\tsess = tf.Session()\n",
    "\tK.set_session(sess)\n",
    "\tenv = gym.make(\"Pendulum-v0\")\n",
    "\tactor_critic = ActorCritic(env, sess)\n",
    "    \n",
    "    # output current NN architecture to .png file\n",
    "\tplot_model(actor_critic.critic_model, to_file='toy_critic.png', show_shapes=True)\n",
    "\tplot_model(actor_critic.actor_model, to_file='toy_actor.png', show_shapes=True)\n",
    "\n",
    "\tnum_trials = 10000\n",
    "\ttrial_len  = 500\n",
    "\n",
    "\tcur_state = env.reset()\n",
    "\taction = env.action_space.sample()\n",
    "\t'''\n",
    "\twhile True:\n",
    "\t\tenv.render()\n",
    "\t\tcur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "\t\taction = actor_critic.act(cur_state)\n",
    "\t\taction = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\tnew_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "\t\tactor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "\t\tactor_critic.train()\n",
    "\n",
    "\t\tcur_state = new_state\n",
    "\t'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env class\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "class PendulumEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes' : ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : 30\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_speed=8\n",
    "        self.max_torque=2.\n",
    "        self.dt=.05\n",
    "        self.viewer = None\n",
    "\n",
    "        high = np.array([1., 1., self.max_speed])\n",
    "        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self,u):\n",
    "        th, thdot = self.state # th := theta\n",
    "\n",
    "        g = 10.\n",
    "        m = 1.\n",
    "        l = 1.\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u # for rendering\n",
    "        costs = angle_normalize(th)**2 + .1*thdot**2 + .001*(u**2)\n",
    "\n",
    "        newthdot = thdot + (-3*g/(2*l) * np.sin(th + np.pi) + 3./(m*l**2)*u) * dt\n",
    "        newth = th + newthdot*dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed) #pylint: disable=E1111\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "        return self._get_obs(), -costs, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        high = np.array([np.pi, 1])\n",
    "        self.state = self.np_random.uniform(low=-high, high=high)\n",
    "        self.last_u = None\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(500,500)\n",
    "            self.viewer.set_bounds(-2.2,2.2,-2.2,2.2)\n",
    "            rod = rendering.make_capsule(1, .2)\n",
    "            rod.set_color(.8, .3, .3)\n",
    "            self.pole_transform = rendering.Transform()\n",
    "            rod.add_attr(self.pole_transform)\n",
    "            self.viewer.add_geom(rod)\n",
    "            axle = rendering.make_circle(.05)\n",
    "            axle.set_color(0,0,0)\n",
    "            self.viewer.add_geom(axle)\n",
    "            fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
    "            self.img = rendering.Image(fname, 1., 1.)\n",
    "            self.imgtrans = rendering.Transform()\n",
    "            self.img.add_attr(self.imgtrans)\n",
    "\n",
    "        self.viewer.add_onetime(self.img)\n",
    "        self.pole_transform.set_rotation(self.state[0] + np.pi/2)\n",
    "        if self.last_u:\n",
    "            self.imgtrans.scale = (-self.last_u/2, np.abs(self.last_u)/2)\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_shape/obs_shape class\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Space(object):\n",
    "    \"\"\"Defines the observation and action spaces, so you can write generic\n",
    "    code that applies to any Env. For example, you can choose a random\n",
    "    action.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        import numpy as np # takes about 300-400ms to import, so we load lazily\n",
    "        self.shape = None if shape is None else tuple(shape)\n",
    "        self.dtype = None if dtype is None else np.dtype(dtype)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Uniformly randomly sample a random element of this space\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def seed(self, seed):\n",
    "        \"\"\"Set the seed for this space's pseudo-random number generator. \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def contains(self, x):\n",
    "        \"\"\"\n",
    "        Return boolean specifying if x is a valid\n",
    "        member of this space\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __contains__(self, x):\n",
    "        return self.contains(x)\n",
    "\n",
    "    def to_jsonable(self, sample_n):\n",
    "        \"\"\"Convert a batch of samples from this space to a JSONable data type.\"\"\"\n",
    "        # By default, assume identity is JSONable\n",
    "        return sample_n\n",
    "\n",
    "    def from_jsonable(self, sample_n):\n",
    "        \"\"\"Convert a JSONable data type to a batch of samples from this space.\"\"\"\n",
    "        # By default, assume identity is JSONable\n",
    "        return sample_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import logger\n",
    "from .space import Space\n",
    "\n",
    "\n",
    "class Box(Space):\n",
    "    \"\"\"\n",
    "    A box in R^n.\n",
    "    I.e., each coordinate is bounded.\n",
    "    Example usage:\n",
    "    self.action_space = spaces.Box(low=-10, high=10, shape=(1,))\n",
    "    \"\"\"\n",
    "    def __init__(self, low=None, high=None, shape=None, dtype=None):\n",
    "        \"\"\"\n",
    "        Two kinds of valid input:\n",
    "            Box(low=-1.0, high=1.0, shape=(3,4)) # low and high are scalars, and shape is provided\n",
    "            Box(low=np.array([-1.0,-2.0]), high=np.array([2.0,4.0])) # low and high are arrays of the same shape\n",
    "        \"\"\"\n",
    "        if shape is None:\n",
    "            assert low.shape == high.shape\n",
    "            shape = low.shape\n",
    "        else:\n",
    "            assert np.isscalar(low) and np.isscalar(high)\n",
    "            low = low + np.zeros(shape)\n",
    "            high = high + np.zeros(shape)\n",
    "        if dtype is None:  # Autodetect type\n",
    "            if (high == 255).all():\n",
    "                dtype = np.uint8\n",
    "            else:\n",
    "                dtype = np.float32\n",
    "            logger.warn(\"gym.spaces.Box autodetected dtype as {}. Please provide explicit dtype.\".format(dtype))\n",
    "        self.low = low.astype(dtype)\n",
    "        self.high = high.astype(dtype)\n",
    "        super(Box, self).__init__(shape, dtype)\n",
    "        self.np_random = np.random.RandomState()\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.np_random.seed(seed)\n",
    "\n",
    "    def sample(self):\n",
    "        high = self.high if self.dtype.kind == 'f' else self.high.astype('int64') + 1\n",
    "        return self.np_random.uniform(low=self.low, high=high, size=self.low.shape).astype(self.dtype)\n",
    "\n",
    "    def contains(self, x):\n",
    "        return x.shape == self.shape and (x >= self.low).all() and (x <= self.high).all()\n",
    "\n",
    "    def to_jsonable(self, sample_n):\n",
    "        return np.array(sample_n).tolist()\n",
    "\n",
    "    def from_jsonable(self, sample_n):\n",
    "        return [np.asarray(sample) for sample in sample_n]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Box\" + str(self.shape)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return np.allclose(self.low, other.low) and np.allclose(self.high, other.high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
