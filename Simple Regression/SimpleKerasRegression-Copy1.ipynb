{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "# modified from \n",
    "# https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 1000000\n",
    "n_partitions = 10\n",
    "\n",
    "# Generate tasks and partitions\n",
    "partition_data = np.random.random((n_samples, n_partitions)) # partition data generation\n",
    "\n",
    "#X = np.zeros((n_samples, n_partitions + 1))             # initialize input layer\n",
    "y = np.zeros((n_samples, n_partitions))                 # initialize outputs layer for training \n",
    "\n",
    "task_data = np.zeros((n_samples, 1))                         # initialize task list\n",
    "\n",
    "for i in range (0, n_samples):\n",
    "    \n",
    "    partitions = partition_data[i]\n",
    "    task = random.uniform(0, partitions.max())\n",
    "    task_data[i] = task\n",
    "    \n",
    "    for j in range (0, n_partitions):\n",
    "        current_fit = partitions[j] - task\n",
    "        y[i,j] = current_fit\n",
    "            \n",
    "X = np.hstack((task_data,partition_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data between train and test set \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "n_hidden_units = 500\n",
    "# Dense(n_hidden_units) is a fully-connected layer with n_hidden_units hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 10-dimensional vectors.\n",
    "model.add(Dense(n_hidden_units, activation='elu', input_dim=n_partitions+1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_hidden_units, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='elu'))\n",
    "\n",
    "# optimizer options\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = tf.train.RMSPropOptimizer(0.008)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=sgd,\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "800000/800000 [==============================] - 27s 34us/step - loss: 0.0251 - mean_absolute_error: 0.1162\n",
      "Epoch 2/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0121 - mean_absolute_error: 0.0865\n",
      "Epoch 3/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0103 - mean_absolute_error: 0.0798\n",
      "Epoch 4/120\n",
      "800000/800000 [==============================] - 30s 38us/step - loss: 0.0094 - mean_absolute_error: 0.0760\n",
      "Epoch 5/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0089 - mean_absolute_error: 0.0737\n",
      "Epoch 6/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0086 - mean_absolute_error: 0.0722\n",
      "Epoch 7/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0083 - mean_absolute_error: 0.0710\n",
      "Epoch 8/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0081 - mean_absolute_error: 0.0701\n",
      "Epoch 9/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0080 - mean_absolute_error: 0.0694\n",
      "Epoch 10/120\n",
      "800000/800000 [==============================] - 28s 35us/step - loss: 0.0078 - mean_absolute_error: 0.0688\n",
      "Epoch 11/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0077 - mean_absolute_error: 0.0684\n",
      "Epoch 12/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0077 - mean_absolute_error: 0.0680\n",
      "Epoch 13/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0076 - mean_absolute_error: 0.0676\n",
      "Epoch 14/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0075 - mean_absolute_error: 0.0673\n",
      "Epoch 15/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0075 - mean_absolute_error: 0.0671\n",
      "Epoch 16/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0074 - mean_absolute_error: 0.0668\n",
      "Epoch 17/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0074 - mean_absolute_error: 0.0666\n",
      "Epoch 18/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0074 - mean_absolute_error: 0.0664\n",
      "Epoch 19/120\n",
      "800000/800000 [==============================] - 34s 42us/step - loss: 0.0073 - mean_absolute_error: 0.0662\n",
      "Epoch 20/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0073 - mean_absolute_error: 0.0661\n",
      "Epoch 21/120\n",
      "800000/800000 [==============================] - 29s 37us/step - loss: 0.0073 - mean_absolute_error: 0.0659\n",
      "Epoch 22/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0072 - mean_absolute_error: 0.0658\n",
      "Epoch 23/120\n",
      "800000/800000 [==============================] - 29s 37us/step - loss: 0.0072 - mean_absolute_error: 0.0657\n",
      "Epoch 24/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0072 - mean_absolute_error: 0.0655\n",
      "Epoch 25/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0072 - mean_absolute_error: 0.0655\n",
      "Epoch 26/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0071 - mean_absolute_error: 0.0653\n",
      "Epoch 27/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0071 - mean_absolute_error: 0.0652\n",
      "Epoch 28/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0071 - mean_absolute_error: 0.0651\n",
      "Epoch 29/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0071 - mean_absolute_error: 0.0651\n",
      "Epoch 30/120\n",
      "800000/800000 [==============================] - 29s 37us/step - loss: 0.0071 - mean_absolute_error: 0.0650\n",
      "Epoch 31/120\n",
      "800000/800000 [==============================] - 30s 38us/step - loss: 0.0070 - mean_absolute_error: 0.0649\n",
      "Epoch 32/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0070 - mean_absolute_error: 0.0648\n",
      "Epoch 33/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0070 - mean_absolute_error: 0.0647\n",
      "Epoch 34/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0070 - mean_absolute_error: 0.0646\n",
      "Epoch 35/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0070 - mean_absolute_error: 0.0646\n",
      "Epoch 36/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0070 - mean_absolute_error: 0.0645\n",
      "Epoch 37/120\n",
      "800000/800000 [==============================] - 28s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0644\n",
      "Epoch 38/120\n",
      "800000/800000 [==============================] - 28s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0644 1s - loss: 0.0069 - mean_\n",
      "Epoch 39/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0643\n",
      "Epoch 40/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0642\n",
      "Epoch 41/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0642\n",
      "Epoch 42/120\n",
      "800000/800000 [==============================] - 29s 37us/step - loss: 0.0069 - mean_absolute_error: 0.0642\n",
      "Epoch 43/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0641\n",
      "Epoch 44/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0640\n",
      "Epoch 45/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0069 - mean_absolute_error: 0.0640\n",
      "Epoch 46/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0069 - mean_absolute_error: 0.0639\n",
      "Epoch 47/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0639\n",
      "Epoch 48/120\n",
      "800000/800000 [==============================] - 30s 38us/step - loss: 0.0068 - mean_absolute_error: 0.0638\n",
      "Epoch 49/120\n",
      "800000/800000 [==============================] - 29s 37us/step - loss: 0.0068 - mean_absolute_error: 0.0638\n",
      "Epoch 50/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0068 - mean_absolute_error: 0.0637\n",
      "Epoch 51/120\n",
      "800000/800000 [==============================] - 29s 37us/step - loss: 0.0068 - mean_absolute_error: 0.0637\n",
      "Epoch 52/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0636\n",
      "Epoch 53/120\n",
      "800000/800000 [==============================] - 28s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0636\n",
      "Epoch 54/120\n",
      "800000/800000 [==============================] - 28s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0636\n",
      "Epoch 55/120\n",
      "800000/800000 [==============================] - 28s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0635\n",
      "Epoch 56/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0635\n",
      "Epoch 57/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0068 - mean_absolute_error: 0.0634\n",
      "Epoch 58/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0634\n",
      "Epoch 59/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0634\n",
      "Epoch 60/120\n",
      "800000/800000 [==============================] - 30s 37us/step - loss: 0.0067 - mean_absolute_error: 0.0633\n",
      "Epoch 61/120\n",
      "800000/800000 [==============================] - 33s 41us/step - loss: 0.0067 - mean_absolute_error: 0.0632\n",
      "Epoch 62/120\n",
      "800000/800000 [==============================] - 32s 40us/step - loss: 0.0067 - mean_absolute_error: 0.0632\n",
      "Epoch 63/120\n",
      "800000/800000 [==============================] - 30s 38us/step - loss: 0.0067 - mean_absolute_error: 0.0632\n",
      "Epoch 64/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0631\n",
      "Epoch 65/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0067 - mean_absolute_error: 0.0631\n",
      "Epoch 66/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0067 - mean_absolute_error: 0.0631\n",
      "Epoch 67/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0630\n",
      "Epoch 68/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0630\n",
      "Epoch 69/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0630\n",
      "Epoch 70/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0067 - mean_absolute_error: 0.0629\n",
      "Epoch 71/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0066 - mean_absolute_error: 0.0629\n",
      "Epoch 72/120\n",
      "800000/800000 [==============================] - 30s 38us/step - loss: 0.0066 - mean_absolute_error: 0.0628\n",
      "Epoch 73/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0066 - mean_absolute_error: 0.0628\n",
      "Epoch 74/120\n",
      "800000/800000 [==============================] - 31s 38us/step - loss: 0.0066 - mean_absolute_error: 0.0628\n",
      "Epoch 75/120\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 0.0066 - mean_absolute_error: 0.0628\n",
      "Epoch 76/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0066 - mean_absolute_error: 0.0627\n",
      "Epoch 77/120\n",
      "800000/800000 [==============================] - 32s 41us/step - loss: 0.0066 - mean_absolute_error: 0.0627\n",
      "Epoch 78/120\n",
      "800000/800000 [==============================] - 32s 40us/step - loss: 0.0066 - mean_absolute_error: 0.0626\n",
      "Epoch 79/120\n",
      "800000/800000 [==============================] - 35s 44us/step - loss: 0.0066 - mean_absolute_error: 0.0626\n",
      "Epoch 80/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0066 - mean_absolute_error: 0.0626\n",
      "Epoch 81/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0066 - mean_absolute_error: 0.0625\n",
      "Epoch 82/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0066 - mean_absolute_error: 0.0625\n",
      "Epoch 83/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0066 - mean_absolute_error: 0.0625\n",
      "Epoch 84/120\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0066 - mean_absolute_error: 0.0624\n",
      "Epoch 85/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0066 - mean_absolute_error: 0.0624\n",
      "Epoch 86/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0066 - mean_absolute_error: 0.0624\n",
      "Epoch 87/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0623\n",
      "Epoch 88/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0623\n",
      "Epoch 89/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0623\n",
      "Epoch 90/120\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0622\n",
      "Epoch 91/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0622\n",
      "Epoch 92/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0622 5s - loss: 0.0065 - mean\n",
      "Epoch 93/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0622\n",
      "Epoch 94/120\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0621 3s - loss: 0.0065 - mean_absolute_error\n",
      "Epoch 95/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0621\n",
      "Epoch 96/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0621\n",
      "Epoch 97/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0621\n",
      "Epoch 98/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0620\n",
      "Epoch 99/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0620\n",
      "Epoch 100/120\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0620\n",
      "Epoch 101/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0620\n",
      "Epoch 102/120\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0065 - mean_absolute_error: 0.0619\n",
      "Epoch 103/120\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0619\n",
      "Epoch 104/120\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0065 - mean_absolute_error: 0.0619\n",
      "Epoch 105/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0618 1s - loss: 0.0065 - \n",
      "Epoch 106/120\n",
      "800000/800000 [==============================] - 37s 47us/step - loss: 0.0064 - mean_absolute_error: 0.0618\n",
      "Epoch 107/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0618\n",
      "Epoch 108/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0618\n",
      "Epoch 109/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0618\n",
      "Epoch 110/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0617\n",
      "Epoch 111/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0617\n",
      "Epoch 112/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0616\n",
      "Epoch 113/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0616\n",
      "Epoch 114/120\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0064 - mean_absolute_error: 0.0616\n",
      "Epoch 115/120\n",
      "800000/800000 [==============================] - 37s 47us/step - loss: 0.0064 - mean_absolute_error: 0.0616 2s - loss: 0.0064 - mean_absolute_error: 0 - ETA: 2s - loss: 0.0064 - mea - ETA: 0s - loss: 0.0064 - mean_absolute_er\n",
      "Epoch 116/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0064 - mean_absolute_error: 0.0616\n",
      "Epoch 117/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0064 - mean_absolute_error: 0.0615\n",
      "Epoch 118/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0064 - mean_absolute_error: 0.0615\n",
      "Epoch 119/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0064 - mean_absolute_error: 0.0615\n",
      "Epoch 120/120\n",
      "800000/800000 [==============================] - 31s 39us/step - loss: 0.0064 - mean_absolute_error: 0.0615\n",
      "200000/200000 [==============================] - 2s 11us/step\n",
      "[0.001988360576964915, 0.026650526580810546]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhy is the training loss much higher than the testing loss?\\n\\nA Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and \\nL1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average \\nof the losses over each batch of training data. Because your model is changing over time, the loss over \\nthe first batches of an epoch is generally higher than over the last batches. On the other hand, the \\ntesting loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize = 512\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=120,\n",
    "          batch_size=batchsize)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batchsize)\n",
    "print(score)\n",
    "\n",
    "'''\n",
    "Why is the training loss much higher than the testing loss?\n",
    "\n",
    "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and \n",
    "L1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average \n",
    "of the losses over each batch of training data. Because your model is changing over time, the loss over \n",
    "the first batches of an epoch is generally higher than over the last batches. On the other hand, the \n",
    "testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndel model  # deletes the existing model\\n\\n# returns a compiled model\\n# identical to the previous one\\nmodel = load_model('MLP_Multiclass_softmax_10_inputs.h5')\\nmodel.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\\nmodel.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('MLP_elu_softmax_11_inputs_v2.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model.save_weights('MLP_elu_softmax_11_inputs_weights_v2.h5')\n",
    "\n",
    "'''\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('MLP_Multiclass_softmax_10_inputs.h5')\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
