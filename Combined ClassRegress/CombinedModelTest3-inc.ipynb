{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "# modified from \n",
    "# https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 1000000\n",
    "n_partitions = 10\n",
    "\n",
    "# Generate tasks and partitions\n",
    "partition_data = np.random.random((n_samples, n_partitions)) # partition data generation\n",
    "\n",
    "data = np.zeros((n_samples, n_partitions))                   # initialize input layer\n",
    "labels = np.zeros((n_samples, n_partitions))                 # initialize outputs layer for training \n",
    "\n",
    "task_data = np.zeros((n_samples, 1))                         # initialize task list\n",
    "\n",
    "# fails to account a 'perfect' fit where fit == 0\n",
    "for i in range (0, n_samples):\n",
    "    \n",
    "    partitions = partition_data[i]\n",
    "    task = random.uniform(0, partitions.max())\n",
    "    task_data[i] = task\n",
    "    \n",
    "    best_partition = -1\n",
    "    best_fit = 999999999\n",
    "    \n",
    "    for j in range (0, n_partitions):\n",
    "        current_fit = partitions[j] - task\n",
    "        data[i,j] = current_fit\n",
    "        if current_fit > 0 and current_fit < best_fit:\n",
    "            best_fit = current_fit\n",
    "            best_partition = j\n",
    "    \n",
    "    labels[i][best_partition] = 1\n",
    "    \n",
    "\n",
    "X = np.hstack((task_data,partition_data))\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data between train and test set \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('combined1.h5')\n",
    "\n",
    "# optimizer options\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = tf.train.RMSPropOptimizer(0.008)\n",
    "adam = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800000/800000 [==============================] - 38s 47us/step - loss: 1.3788 - acc: 0.4169\n",
      "Epoch 2/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3790 - acc: 0.4190\n",
      "Epoch 3/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3794 - acc: 0.4189\n",
      "Epoch 4/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3783 - acc: 0.4199\n",
      "Epoch 5/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3786 - acc: 0.4201\n",
      "Epoch 6/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3781 - acc: 0.4207\n",
      "Epoch 7/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3777 - acc: 0.4207\n",
      "Epoch 8/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3787 - acc: 0.4206\n",
      "Epoch 9/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3788 - acc: 0.4208\n",
      "Epoch 10/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3785 - acc: 0.4209\n",
      "Epoch 11/100\n",
      "800000/800000 [==============================] - 40s 51us/step - loss: 1.3799 - acc: 0.4205\n",
      "Epoch 12/100\n",
      "800000/800000 [==============================] - 40s 51us/step - loss: 1.3786 - acc: 0.4216\n",
      "Epoch 13/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3790 - acc: 0.4212\n",
      "Epoch 14/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3782 - acc: 0.4213\n",
      "Epoch 15/100\n",
      "800000/800000 [==============================] - 40s 51us/step - loss: 1.3776 - acc: 0.4218\n",
      "Epoch 16/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3776 - acc: 0.4225\n",
      "Epoch 17/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3769 - acc: 0.4226\n",
      "Epoch 18/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3762 - acc: 0.4218\n",
      "Epoch 19/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3767 - acc: 0.4225\n",
      "Epoch 20/100\n",
      "800000/800000 [==============================] - 40s 51us/step - loss: 1.3787 - acc: 0.4213\n",
      "Epoch 21/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3770 - acc: 0.4218\n",
      "Epoch 22/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3757 - acc: 0.4239\n",
      "Epoch 23/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3763 - acc: 0.4220\n",
      "Epoch 24/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3764 - acc: 0.4227\n",
      "Epoch 25/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3757 - acc: 0.4233\n",
      "Epoch 26/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3765 - acc: 0.4224\n",
      "Epoch 27/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3757 - acc: 0.4230\n",
      "Epoch 28/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3772 - acc: 0.4214\n",
      "Epoch 29/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3769 - acc: 0.4227\n",
      "Epoch 30/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3756 - acc: 0.4225\n",
      "Epoch 31/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3773 - acc: 0.4223\n",
      "Epoch 32/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3760 - acc: 0.4227\n",
      "Epoch 33/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3753 - acc: 0.4231\n",
      "Epoch 34/100\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 1.3756 - acc: 0.4231\n",
      "Epoch 35/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3765 - acc: 0.4227\n",
      "Epoch 36/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3752 - acc: 0.4229\n",
      "Epoch 37/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3755 - acc: 0.4219\n",
      "Epoch 38/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3749 - acc: 0.4231\n",
      "Epoch 39/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3767 - acc: 0.4232\n",
      "Epoch 40/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3751 - acc: 0.4234\n",
      "Epoch 41/100\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 1.3740 - acc: 0.4246\n",
      "Epoch 42/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3760 - acc: 0.4229\n",
      "Epoch 43/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3743 - acc: 0.4243\n",
      "Epoch 44/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3764 - acc: 0.4224\n",
      "Epoch 45/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3752 - acc: 0.4228\n",
      "Epoch 46/100\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 1.3759 - acc: 0.4231\n",
      "Epoch 47/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3753 - acc: 0.4239\n",
      "Epoch 48/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3751 - acc: 0.4237\n",
      "Epoch 49/100\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 1.3757 - acc: 0.4228\n",
      "Epoch 50/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3759 - acc: 0.4239\n",
      "Epoch 51/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3745 - acc: 0.4241\n",
      "Epoch 52/100\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 1.3754 - acc: 0.4234\n",
      "Epoch 53/100\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 1.3747 - acc: 0.4229\n",
      "Epoch 54/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3749 - acc: 0.4226\n",
      "Epoch 55/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3758 - acc: 0.4236\n",
      "Epoch 56/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3757 - acc: 0.4225\n",
      "Epoch 57/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3742 - acc: 0.4237\n",
      "Epoch 58/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3749 - acc: 0.4236\n",
      "Epoch 59/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3746 - acc: 0.4242\n",
      "Epoch 60/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3738 - acc: 0.4235\n",
      "Epoch 61/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3734 - acc: 0.4238\n",
      "Epoch 62/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3756 - acc: 0.4234\n",
      "Epoch 63/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3747 - acc: 0.4230\n",
      "Epoch 64/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3745 - acc: 0.4243\n",
      "Epoch 65/100\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 1.3738 - acc: 0.4242\n",
      "Epoch 66/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3748 - acc: 0.4240\n",
      "Epoch 67/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3756 - acc: 0.4231\n",
      "Epoch 68/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3742 - acc: 0.4236\n",
      "Epoch 69/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3742 - acc: 0.4240\n",
      "Epoch 70/100\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 1.3761 - acc: 0.4237\n",
      "Epoch 71/100\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 1.3745 - acc: 0.4238\n",
      "Epoch 72/100\n",
      "800000/800000 [==============================] - 39s 48us/step - loss: 1.3740 - acc: 0.4241\n",
      "Epoch 73/100\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 1.3751 - acc: 0.4229\n",
      "Epoch 74/100\n",
      "465000/800000 [================>.............] - ETA: 16s - loss: 1.3744 - acc: 0.4246"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d7255db7ea25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m model.fit(X_train, y_train,\n\u001b[0;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           batch_size=batchsize)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batchsize = 100\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=100,\n",
    "          batch_size=batchsize)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batchsize)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Why is the training loss much higher than the testing loss?\n",
    "\n",
    "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and \n",
    "L1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average \n",
    "of the losses over each batch of training data. Because your model is changing over time, the loss over \n",
    "the first batches of an epoch is generally higher than over the last batches. On the other hand, the \n",
    "testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('combined2.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model.save_weights('combined2_weights.h5')\n",
    "\n",
    "'''\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('MLP_Multiclass_softmax_10_inputs.h5')\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
