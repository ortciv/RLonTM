{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "# modified from \n",
    "# https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 1000000\n",
    "n_partitions = 10\n",
    "\n",
    "# Generate tasks and partitions\n",
    "partition_data = np.random.random((n_samples, n_partitions)) # partition data generation\n",
    "\n",
    "#X = np.zeros((n_samples, n_partitions + 1))             # initialize input layer\n",
    "y = np.zeros((n_samples, n_partitions))                 # initialize outputs layer for training \n",
    "\n",
    "task_data = np.zeros((n_samples, 1))                         # initialize task list\n",
    "\n",
    "for i in range (0, n_samples):\n",
    "    \n",
    "    partitions = partition_data[i]\n",
    "    task = random.uniform(0, partitions.max())\n",
    "    task_data[i] = task\n",
    "    \n",
    "    for j in range (0, n_partitions):\n",
    "        current_fit = partitions[j] - task\n",
    "        y[i,j] = current_fit\n",
    "            \n",
    "X = np.hstack((task_data,partition_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data between train and test set \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "n_hidden_units = 256\n",
    "# Dense(n_hidden_units) is a fully-connected layer with n_hidden_units hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 10-dimensional vectors.\n",
    "model.add(Dense(n_hidden_units, activation='relu', input_dim=10))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_hidden_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# optimizer options\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = tf.train.RMSPropOptimizer(0.008)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "800000/800000 [==============================] - 26s 32us/step - loss: 2.1310 - acc: 0.1860\n",
      "Epoch 2/40\n",
      "800000/800000 [==============================] - 25s 31us/step - loss: 1.7300 - acc: 0.3168\n",
      "Epoch 3/40\n",
      "800000/800000 [==============================] - 26s 33us/step - loss: 1.4578 - acc: 0.4148\n",
      "Epoch 4/40\n",
      "800000/800000 [==============================] - 26s 32us/step - loss: 1.3177 - acc: 0.4677\n",
      "Epoch 5/40\n",
      "800000/800000 [==============================] - 24s 31us/step - loss: 1.1972 - acc: 0.5144\n",
      "Epoch 6/40\n",
      "800000/800000 [==============================] - 24s 30us/step - loss: 1.0823 - acc: 0.5600\n",
      "Epoch 7/40\n",
      "800000/800000 [==============================] - 23s 29us/step - loss: 0.9920 - acc: 0.5963\n",
      "Epoch 8/40\n",
      "800000/800000 [==============================] - 25s 32us/step - loss: 0.9220 - acc: 0.6247\n",
      "Epoch 9/40\n",
      "800000/800000 [==============================] - 26s 33us/step - loss: 0.8750 - acc: 0.6434\n",
      "Epoch 10/40\n",
      "800000/800000 [==============================] - 26s 33us/step - loss: 0.8408 - acc: 0.6571\n",
      "Epoch 11/40\n",
      "800000/800000 [==============================] - 26s 33us/step - loss: 0.8159 - acc: 0.6675\n",
      "Epoch 12/40\n",
      "800000/800000 [==============================] - 26s 32us/step - loss: 0.7964 - acc: 0.6758\n",
      "Epoch 13/40\n",
      "800000/800000 [==============================] - 27s 33us/step - loss: 0.7807 - acc: 0.6829\n",
      "Epoch 14/40\n",
      "800000/800000 [==============================] - 25s 32us/step - loss: 0.7678 - acc: 0.6882\n",
      "Epoch 15/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.7561 - acc: 0.6941\n",
      "Epoch 16/40\n",
      "800000/800000 [==============================] - 22s 28us/step - loss: 0.7478 - acc: 0.6970\n",
      "Epoch 17/40\n",
      "800000/800000 [==============================] - 24s 29us/step - loss: 0.7403 - acc: 0.7012\n",
      "Epoch 18/40\n",
      "800000/800000 [==============================] - 25s 31us/step - loss: 0.7335 - acc: 0.7040\n",
      "Epoch 19/40\n",
      "800000/800000 [==============================] - 24s 30us/step - loss: 0.7275 - acc: 0.7072\n",
      "Epoch 20/40\n",
      "800000/800000 [==============================] - 24s 30us/step - loss: 0.7217 - acc: 0.7101\n",
      "Epoch 21/40\n",
      "800000/800000 [==============================] - 23s 29us/step - loss: 0.7170 - acc: 0.7123\n",
      "Epoch 22/40\n",
      "800000/800000 [==============================] - 22s 27us/step - loss: 0.7129 - acc: 0.7142\n",
      "Epoch 23/40\n",
      "800000/800000 [==============================] - 25s 31us/step - loss: 0.7084 - acc: 0.7161\n",
      "Epoch 24/40\n",
      "800000/800000 [==============================] - 23s 29us/step - loss: 0.7043 - acc: 0.7191\n",
      "Epoch 25/40\n",
      "800000/800000 [==============================] - 23s 28us/step - loss: 0.7016 - acc: 0.7200\n",
      "Epoch 26/40\n",
      "800000/800000 [==============================] - 23s 28us/step - loss: 0.6992 - acc: 0.7203\n",
      "Epoch 27/40\n",
      "800000/800000 [==============================] - 22s 27us/step - loss: 0.6964 - acc: 0.7224\n",
      "Epoch 28/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.6922 - acc: 0.7237\n",
      "Epoch 29/40\n",
      "800000/800000 [==============================] - 20s 25us/step - loss: 0.6903 - acc: 0.7255\n",
      "Epoch 30/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.6893 - acc: 0.7255\n",
      "Epoch 31/40\n",
      "800000/800000 [==============================] - 22s 27us/step - loss: 0.6853 - acc: 0.7274\n",
      "Epoch 32/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.6837 - acc: 0.7279\n",
      "Epoch 33/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.6821 - acc: 0.7296\n",
      "Epoch 34/40\n",
      "800000/800000 [==============================] - 22s 27us/step - loss: 0.6788 - acc: 0.7312\n",
      "Epoch 35/40\n",
      "800000/800000 [==============================] - 20s 25us/step - loss: 0.6776 - acc: 0.7308\n",
      "Epoch 36/40\n",
      "800000/800000 [==============================] - 22s 27us/step - loss: 0.6750 - acc: 0.7324\n",
      "Epoch 37/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.6747 - acc: 0.7333\n",
      "Epoch 38/40\n",
      "800000/800000 [==============================] - 21s 26us/step - loss: 0.6725 - acc: 0.7336\n",
      "Epoch 39/40\n",
      "800000/800000 [==============================] - 20s 25us/step - loss: 0.6711 - acc: 0.7340\n",
      "Epoch 40/40\n",
      "800000/800000 [==============================] - 19s 24us/step - loss: 0.6696 - acc: 0.7351\n",
      "200000/200000 [==============================] - 1s 6us/step\n",
      "[0.4420879859352112, 0.8510299999809265]\n"
     ]
    }
   ],
   "source": [
    "batchsize = 512\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=40,\n",
    "          batch_size=batchsize)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batchsize)\n",
    "print(score)\n",
    "\n",
    "'''\n",
    "Why is the training loss much higher than the testing loss?\n",
    "\n",
    "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and \n",
    "L1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average \n",
    "of the losses over each batch of training data. Because your model is changing over time, the loss over \n",
    "the first batches of an epoch is generally higher than over the last batches. On the other hand, the \n",
    "testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndel model  # deletes the existing model\\n\\n# returns a compiled model\\n# identical to the previous one\\nmodel = load_model('MLP_Multiclass_softmax_10_inputs.h5')\\nmodel.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\\nmodel.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('MLP_Multiclass_softmax_10_inputs.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model.save_weights('MLP_Multiclass_softmax_10_inputs_weights.h5')\n",
    "\n",
    "'''\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('MLP_Multiclass_softmax_10_inputs.h5')\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
