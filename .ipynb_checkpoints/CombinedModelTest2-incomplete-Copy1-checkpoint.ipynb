{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "# modified from \n",
    "# https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 1000000\n",
    "n_partitions = 10\n",
    "\n",
    "# Generate tasks and partitions\n",
    "partition_data = np.random.random((n_samples, n_partitions)) # partition data generation\n",
    "\n",
    "data = np.zeros((n_samples, n_partitions))                   # initialize input layer\n",
    "labels = np.zeros((n_samples, n_partitions))                 # initialize outputs layer for training \n",
    "\n",
    "task_data = np.zeros((n_samples, 1))                         # initialize task list\n",
    "\n",
    "# fails to account a 'perfect' fit where fit == 0\n",
    "for i in range (0, n_samples):\n",
    "    \n",
    "    partitions = partition_data[i]\n",
    "    task = random.uniform(0, partitions.max())\n",
    "    task_data[i] = task\n",
    "    \n",
    "    best_partition = -1\n",
    "    best_fit = 999999999\n",
    "    \n",
    "    for j in range (0, n_partitions):\n",
    "        current_fit = partitions[j] - task\n",
    "        data[i,j] = current_fit\n",
    "        if current_fit > 0 and current_fit < best_fit:\n",
    "            best_fit = current_fit\n",
    "            best_partition = j\n",
    "    \n",
    "    labels[i][best_partition] = 1\n",
    "    \n",
    "\n",
    "X = np.hstack((task_data,partition_data))\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data between train and test set \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('MLP_elu_softmax_11_inputs.h5')\n",
    "\n",
    "# Freeze all layers in first model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construct 2nd half model\n",
    "n_nodes = 250\n",
    "model.add(Dense(n_nodes, activation='relu', name='layerA'))\n",
    "model.add(Dropout(0.5, name='layerA_dropout'))\n",
    "model.add(Dense(n_nodes, activation='relu', name='layerB'))\n",
    "model.add(Dropout(0.5, name='layerB_dropout'))\n",
    "model.add(Dense(n_partitions, activation='softmax', name='softmax'))\n",
    "\n",
    "# optimizer options\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = tf.train.RMSPropOptimizer(0.008)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "from keras.models import load_model\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='combined_model_config.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: 1.6370 - categorical_accuracy: 0.3352\n",
      "Epoch 2/10\n",
      "800000/800000 [==============================] - 26s 33us/step - loss: 1.3676 - categorical_accuracy: 0.4200\n",
      "Epoch 3/10\n",
      "800000/800000 [==============================] - 27s 34us/step - loss: 1.3446 - categorical_accuracy: 0.4299\n",
      "Epoch 4/10\n",
      "800000/800000 [==============================] - 28s 34us/step - loss: 1.3402 - categorical_accuracy: 0.43120s - loss: 1.3403 - categori\n",
      "Epoch 5/10\n",
      "800000/800000 [==============================] - 27s 34us/step - loss: 1.3364 - categorical_accuracy: 0.4337\n",
      "Epoch 6/10\n",
      "800000/800000 [==============================] - 27s 34us/step - loss: 1.3340 - categorical_accuracy: 0.4346\n",
      "Epoch 7/10\n",
      "800000/800000 [==============================] - 27s 34us/step - loss: 1.3339 - categorical_accuracy: 0.4352\n",
      "Epoch 8/10\n",
      "800000/800000 [==============================] - 26s 33us/step - loss: 1.3330 - categorical_accuracy: 0.4348\n",
      "Epoch 9/10\n",
      "800000/800000 [==============================] - 26s 32us/step - loss: 1.3333 - categorical_accuracy: 0.4350\n",
      "Epoch 10/10\n",
      "800000/800000 [==============================] - 26s 32us/step - loss: 1.3323 - categorical_accuracy: 0.4343\n",
      "200000/200000 [==============================] - 3s 13us/step\n",
      "[0.9871243275642395, 0.61977]\n"
     ]
    }
   ],
   "source": [
    "batchsize = 512\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=10,\n",
    "          batch_size=batchsize)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batchsize)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy is the training loss much higher than the testing loss?\\n\\nA Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and \\nL1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average \\nof the losses over each batch of training data. Because your model is changing over time, the loss over \\nthe first batches of an epoch is generally higher than over the last batches. On the other hand, the \\ntesting loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Why is the training loss much higher than the testing loss?\n",
    "\n",
    "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and \n",
    "L1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average \n",
    "of the losses over each batch of training data. Because your model is changing over time, the loss over \n",
    "the first batches of an epoch is generally higher than over the last batches. On the other hand, the \n",
    "testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortci\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:118: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndel model  # deletes the existing model\\n\\n# returns a compiled model\\n# identical to the previous one\\nmodel = load_model('MLP_Multiclass_softmax_10_inputs.h5')\\nmodel.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\\nmodel.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('combined1.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model.save_weights('combined1_weights.h5')\n",
    "\n",
    "'''\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('MLP_Multiclass_softmax_10_inputs.h5')\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5') # for same architecture\n",
    "model.load_weights('MLP_Multiclass_softmax_10_inputs_weights.h5', by_name=True) # for different architecture\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
